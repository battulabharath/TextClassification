# -*- coding: utf-8 -*-
"""DSAssignmentV1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fivHSHMeWYeV_xf7zdg3Vg7tg1xhBRP2
"""

import nltk
import re
nltk.download('gutenberg')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.corpus import gutenberg

#reading seven books as rawtext
rawtext1 = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')
rawtext2 = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')
rawtext3 = nltk.corpus.gutenberg.raw('austen-sense.txt')
rawtext4 = nltk.corpus.gutenberg.raw('blake-poems.txt')
rawtext5 = nltk.corpus.gutenberg.raw('chesterton-ball.txt')
rawtext6 = nltk.corpus.gutenberg.raw('whitman-leaves.txt')
rawtext7 = nltk.corpus.gutenberg.raw('shakespeare-caesar.txt')

#taking author names array
rawtext = [rawtext1,rawtext2,rawtext3,rawtext4,rawtext5,rawtext6,rawtext7]
Authors = ["chesterton-thursday","shakespeare-hamlet","austen","blake","chesterton-ball","whitman","shakespeare-caesar"]

#method to clean up the documents(remove stop words)
def cleanupDoc(s):
     stopset = set(stopwords.words('english'))
     tokens = nltk.word_tokenize(s)
     cleanup = " ".join(filter(lambda word: word not in stopset, s.split()))
     return cleanup

#further clean up the docs
tokenized_sent = []
for i in range(0,(len(rawtext))):
    tokenized_sent.append(nltk.sent_tokenize(rawtext[i]))
    for j in range(len(tokenized_sent[i])):
        clean = []
        clean = re.sub('[^a-zA-Z]', ' ',tokenized_sent[i][j])
        clean = clean.lower()
        clean = cleanupDoc(clean)
        tokenized_sent[i][j] = ''.join(clean)

#labelling the docs with author names
tokenized_sent_label = []
for i in range(0,(len(rawtext))):
    innersentence = []
    for j in range(0,len(tokenized_sent[i])):
        innersentence.append([(tokenized_sent[i][j]),(Authors[i])])   
    tokenized_sent_label.append(innersentence)

#concatenating all docs into the one set of sentences
tokenized_sent_label_concat = []
for i in range(0,(len(rawtext))):
    for j in range(0,len(tokenized_sent[i])):
        tokenized_sent_label_concat.append(tokenized_sent_label[i][j])

#converting it into pandas
import pandas as pd

df = pd.DataFrame(tokenized_sent_label_concat,columns=["Sentences","Authors"])

#info and description of our data frame
df.info()
df.describe()

#head of our cleaned data frame
df.head()

#Counting the sentences for each author
b1 = []
for i in range(0,len(Authors)):
    Author = Authors[i]
    b1.append(df.loc[df['Authors']==str(Author)])

count = []
for i in range(0,(len(b1))):
    count.append(b1[i].shape[0])

#Data Visualization
import seaborn as sns
sns.barplot(Authors,count)

df['Authors'].unique()

#taking the required features from the data
X = df['Sentences']
y = df['Authors']

#building the Bag of Words using Count Vectorizer
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()

X = cv.fit_transform(X)

#checking our vectorized data i.e., sparse matrix
X

#Applying TF - IDF vectorizer on the data
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

#with N_Gram range with Tri-gram
tfidf = TfidfVectorizer(ngram_range=(1,3))

X_tfidf=tfidf.fit_transform(df['Sentences'])
X_tfidf

#splitting the data(using Count Vectorizer) into train test model
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)

#splitting the data into(using TFIDF Vectorizer) into train test model
X_tfidf_train, X_tfidf_test, y_train, y_test = train_test_split(X_tfidf, y,test_size=0.3,random_state=101)

#Multinomial Naives Bayes Model
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()

#fit the naives bayes using count Vectorizer
j=nb.fit(X_train,y_train)

#fitting the naive bayes using tfidf(3-gram) Vectorizer
j_tf = nb.fit(X_tfidf_train,y_train)

j

j_tf

#predictions with tfidf test
predictions_Mnb_tf = nb.predict(X_tfidf_test)

from sklearn.metrics import confusion_matrix,classification_report

print(confusion_matrix(y_test,predictions_Mnb_tf))
print('\n')
print(classification_report(y_test,predictions_Mnb_tf))

#Getting accuracy score using tfidf(3-gram) Vectorizer
from sklearn.metrics import accuracy_score
accuracy_score(y_test,predictions_Mnb_tf)

#Now using the first algorithm
  #Decision Tree ALgorithm with BOW
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree.fit(X_train,y_train)

predictions_dtree = dtree.predict(X_test)

#Getting accuracy score
from sklearn.metrics import accuracy_score
accuracy_score(y_test,predictions_dtree)

#classification report and confusion matrix for the decision tree
from sklearn.metrics import classification_report,confusion_matrix
print(confusion_matrix(y_test,predictions_dtree))
print('\n')
print(classification_report(y_test,predictions_dtree))

#Decision Tree algorithm using Tfidf Vectorizer
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree.fit(X_tfidf_train,y_train)

predictions_dtree_tf = dtree.predict(X_tfidf_test)

#Getting accuracy score
from sklearn.metrics import accuracy_score
accuracy_score(y_test,predictions_dtree_tf)

#NOW using our second algorithm i.e., SVM
#SVM modeling using BOW
from sklearn.svm import SVC
svc_model = SVC()
svc_model.fit(X_train,y_train)

#predictions and generating the classification and confusion report
predictions_svc = svc_model.predict(X_test)
from sklearn.metrics import classification_report,confusion_matrix
print(confusion_matrix(y_test,predictions_svc))
print('\n')
print(classification_report(y_test,predictions_svc))

#Getting accuracy score
from sklearn.metrics import accuracy_score
accuracy_score(y_test,predictions_svc)

#SVM Modelling using TFidf
from sklearn.svm import SVC
svc_model = SVC()
svc_model.fit(X_tfidf_train,y_train)

#predictions and generating the classification and confusion report using tfidf
predictions_svc_tf = svc_model.predict(X_tfidf_test)
from sklearn.metrics import classification_report,confusion_matrix
print(confusion_matrix(y_test,predictions_svc_tf))
print('\n')
print(classification_report(y_test,predictions_svc_tf))

#Getting accuracy score using tfidf
from sklearn.metrics import accuracy_score
accuracy_score(y_test,predictions_svc_tf)

#Now using our third algorithm
#K-Nearest Neightbor using BOW
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train,y_train)

predictions_knn = knn.predict(X_test)

#classification and confusion report
from sklearn.metrics import classification_report,confusion_matrix
print(confusion_matrix(y_test,predictions_knn))
print('\n')
print(classification_report(y_test,predictions_knn))

#Getting accuracy score
from sklearn.metrics import accuracy_score
accuracy_score(y_test,predictions_knn)

#K Nearest Neighbour using TFidf
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_tfidf_train,y_train)

predictions_knn_tf = knn.predict(X_tfidf_test)

#classification and confusion report for KNN using tfidf
from sklearn.metrics import classification_report,confusion_matrix
print(confusion_matrix(y_test,predictions_knn_tf))
print('\n')
print(classification_report(y_test,predictions_knn_tf))

from sklearn.metrics import accuracy_score
accuracy_score(y_test,predictions_knn_tf)

